# -*- coding: utf-8 -*-
"""SubmissionExpert1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/submissionexpert1-023041ce-9f14-4b77-ac65-7d26b9dae4e9.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241007/auto/storage/goog4_request%26X-Goog-Date%3D20241007T150954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8cef0bfecb90a787cf9f0d70bd524f051af38219fc50231bc446ba885b5650965dc5134a7db0c293e713d28c9305fff557c0a17a0b2ae03aa6c8da119f6c6ae8d4efa5542e38f0e45c0fb3ab9d61a542e1e1254c90f6272a33aa3435886d4fe41e2e2228d8f39e6ab2b0eb2aba3e1b9bc59640b7ae15947a4ed7fe037fb92409ce19bec07a3c12c485007eb25f66861dfd4a91f2302ac7d743847cd207e51dd35ce405083a9168de43531727244453ed0e25f584ca7d8826e92aa76db7868769508510126192e49fa5819e22a899279c1827ea7bcc202692b30f514695ed2b5c726af2b67fc463c25c128815201ec075920331060e767d558b67aa3a4294202d
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'telco-customer-churn:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F13996%2F18858%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241007%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241007T150954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D997835a902b5cf4baa19eaa2941fbf1e7a9b7733216deb82dded3ce3ac1689c27adda3578e1c02f3d1ef91407a8cad2a6ac303aa0c2bc22739ad48ac0b9a89f6908f5e6514de0215a0d73b802af2e78d2065f2c55f98a9e3e4717f415b8158b4b1955b03e85623a6d2aade8eec5ac2381c9ad21ffc62d2b842b52adc9b07a028c534505b99061a2d3b1f32e3d63ff6c4beea5a1a76faada578c98a0a6e954281c3c1b8d4f4752d470783d8549fbe67954a6db58b8b5cf88441b73d255750e9453a5e9a8ac44ab250d121670386633e9866cda12d07c40f10a15fffb409794b601e1c1d2a5f3a37ccf2bc60c26613f5cb3c8fb9f9847d157d211dce8c32642c4c'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

file_path = '/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv'
df = pd.read_csv(file_path)

pip install imbalanced-learn

"""import library yang dibutuhkan"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report

"""melihat dataframe"""

df

""" memahami deskripsi variabel pada data"""

df.info()

"""dari info di atas, Jumlah sampel: 7043 entri.
Kolom TotalCharges: Teridentifikasi sebagai object, padahal seharusnya numerik. Perlu dikonversi. nanti akan kita ubah

Melihat informasi statistik
"""

df.describe()

""" mengetahui apakah ada nilai kosong (null/missing value)"""

df.isnull().sum()

"""Tidak ada missing value sejauh ini

Mengelompokkan numerical dan categorical features
"""

categorical_features = [
    'customerID', 'gender', 'Partner', 'Dependents', 'PhoneService',
    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
    'Contract', 'PaperlessBilling', 'PaymentMethod', 'TotalCharges', 'Churn'
]

numerical_features = ['SeniorCitizen', 'tenure', 'MonthlyCharges']

"""melihat histogram masing-masing fitur numerik"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""Kita dapat mengetahui


*   pelanggan yang bukan warga senior (1) lebih banyak daripada yang senior (0)
*   Lama waktu pelanggan berlangganan (dalam bulan) cukup bervariasi
*   dan biaya bulanan/MonthlyCharges yang dibayar oleh pelanggan lebih banyak yang murah daripada berlangganan yang mahal

Loop untuk semua fitur kategorikal
"""

for feature in categorical_features:
    count = df[feature].value_counts()
    percent = 100 * df[feature].value_counts(normalize=True)
    df2 = pd.DataFrame({'Jumlah Sampel': count, 'Persentase (%)': percent.round(1)})

    # Menampilkan dataframe
    print(f"Feature: {feature}")
    print(df2)

    # Plot histogram untuk masing-masing fitur
    plt.figure(figsize=(8, 4))
    count.plot(kind='bar', title=feature)
    plt.ylabel('Jumlah Sampel')
    plt.show()

"""dari visualisasi di atas,

*   Kolom customer id tidak relevan untuk analisis dan pemodelan karena hanya merupakan identifikasi unik pelanggan. Oleh karena itu, kolom ini bisa dihapus.
*   Kolom TotalCharges Saat ini bertipe object, padahal seharusnya bernilai numerik. Kita perlu melakukan konversi tipe data untuk memungkinkan operasi matematis dan analisis lebih lanjut pada kolom ini.

Menghapus kolom customerID
"""

df = df.drop(columns=['customerID'])

"""Melihat kolom customerID apakah sudah terhapus"""

df.info()

"""Mengamati hubungan antar fitur numerik dengan fungsi pairplot()"""

sns.pairplot(df, diag_kind = 'kde')

"""Dari visualisasi di atas, tidak ada pola/trend tertentu, mari lanjut ke tahap berikutnya

sekarang hubungan antar fitur dengan matrix korelasi
"""

plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""*   SeniorCitizen vs. tenure: Korelasi sebesar 0.02, yang sangat rendah, menunjukkan hampir tidak ada hubungan antara menjadi senior citizen dan lama waktu berlangganan (tenure).
*   SeniorCitizen vs. MonthlyCharges: Korelasi sebesar 0.22, yang menunjukkan hubungan yang lemah namun positif. Artinya, meskipun lemah, ada kecenderungan kecil bahwa senior citizen mungkin memiliki tagihan bulanan yang lebih tinggi.
*   tenure vs. MonthlyCharges: Korelasi sebesar 0.25, menunjukkan hubungan yang sedikit lebih kuat namun masih lemah. Ini berarti bahwa semakin lama seseorang menjadi pelanggan, ada kecenderungan kecil mereka memiliki tagihan bulanan yang lebih tinggi.

Secara keseluruhan, korelasi di sini menunjukkan bahwa tidak ada hubungan yang sangat kuat antara variabel-variabel ini.

Melakukan One Hot Encoding untuk semua fitur kategorikal kecuali 'Churn' dan 'TotalCharges'

One Hot Encoding dilakukan untuk memberikan representasi numerik yang diperlukan untuk model machine learning tanpa memberikan bobot tambahan pada kategori tertentu. Dalam hal ini, nilai True menunjukkan kehadiran fitur tertentu, sedangkan False menunjukkan sebaliknya, sehingga memungkinkan model untuk memahami informasi yang lebih baik dalam konteks klasifikasi churn pelanggan.
"""

# Daftar fitur kategorikal, kecuali kolom 'Churn' dan , 'TotalCharges'
categorical_features = [
    'gender', 'Partner', 'Dependents', 'PhoneService',
    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
    'Contract', 'PaperlessBilling', 'PaymentMethod'
]

# Lakukan One Hot Encoding untuk semua fitur kategorikal kecuali 'Churn'
df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=False)

"""melihat info dataframe setelah melakukan one hot encoding"""

df_encoded.info()

"""melihat dataframe setelah melakukan one hot encoding"""

df_encoded

"""Sekarang kita akan mengUbah 'TotalCharges' menjadi numerik

melakukan konversi kolom TotalCharges dari tipe data object menjadi tipe numerik (float64). Proses ini penting karena kolom TotalCharges seharusnya menyimpan nilai numerik yang merepresentasikan total biaya, dan kita perlu melakukan perhitungan matematis pada data tersebut.
"""

df_encoded['TotalCharges'] = pd.to_numeric(df_encoded['TotalCharges'], errors='coerce')

# Cek apakah ada nilai NaN setelah konversi
print(df_encoded['TotalCharges'].isnull().sum())

"""karena terdapat nilai NaN, kita harus menangani data tersebut, lebih baik lihat dulu statistiknya"""

# Cek statistik deskriptif untuk TotalCharges
print(df_encoded['TotalCharges'].describe())

# Visualisasi persebaran data dengan histogram
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df_encoded['TotalCharges'], bins=30, kde=True)
plt.title('Histogram TotalCharges')
plt.xlabel('TotalCharges')
plt.ylabel('Frequency')
plt.show()

# Visualisasi boxplot untuk mendeteksi outlier
plt.figure(figsize=(10, 6))
sns.boxplot(x=df_encoded['TotalCharges'])
plt.title('Boxplot TotalCharges')
plt.show()

"""Dari visualisasi yang telah dilakukan, kita memperoleh beberapa insight mengenai distribusi kolom


1.   Distribusi Right-Skewed:


  *   Distribusi data pada kolom TotalCharges menunjukkan pola right-skewed, di mana sebagian besar nilai terkonsentrasi di sisi kiri (nilai yang lebih rendah), sementara ada sedikit nilai yang lebih tinggi di sisi kanan.
  *   Kondisi ini sering kali terjadi dalam data yang merepresentasikan biaya, di mana sebagian besar pelanggan memiliki biaya rendah dan hanya beberapa yang memiliki biaya sangat tinggi.


2.   Ketiadaan Outlier:

  *   Dari analisis menggunakan boxplot, tidak terdeteksi adanya outlier. Ini menunjukkan bahwa data dalam kolom TotalCharges tidak memiliki nilai ekstrem yang signifikan, yang berarti kita dapat lebih percaya diri dalam melakukan imputasi untuk nilai kosong tanpa terpengaruh oleh nilai-nilai yang tidak biasa.


3.  Penggantian Nilai Kosong dengan Median:


*   Karena distribusi data skewed ke kanan, lebih bijaksana untuk mengganti nilai kosong dengan median daripada rata-rata.
*   Median adalah nilai tengah dari data, yang lebih tahan terhadap kemiringan dan tidak terpengaruh oleh nilai ekstrim. Ini membuat median menjadi pilihan yang lebih baik untuk menggantikan nilai yang hilang dalam konteks data yang tidak terdistribusi normal.




"""

# Ganti nilai kosong di kolom 'TotalCharges' dengan median
df_encoded['TotalCharges'] = pd.to_numeric(df_encoded['TotalCharges'], errors='coerce')
median_totalcharges = df_encoded['TotalCharges'].median()
df_encoded['TotalCharges'].fillna(median_totalcharges, inplace=True)

# Cek kembali apakah masih ada nilai kosong
print(df_encoded['TotalCharges'].isnull().sum())

"""Melihat ringkasan informasi mengenai DataFrame saat ini"""

df_encoded.info()

"""Standarisasi

Standarisasi adalah proses mengubah fitur numerik menjadi skala yang sama, dengan mean 0 dan deviasi standar 1. Ini penting karena:


*   Menyamakan Skala
*   Meningkatkan Kinerja Model
"""

# Tentukan fitur numerik dan biner (hasil One Hot Encoding)
features_to_scale = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']  # Sudah numerik

# Standarisasi data (PCA sensitif terhadap skala data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded[features_to_scale])

"""menyatukan kolom dari standarisasi ke dataframe"""

# Buat DataFrame dari hasil standarisasi
df_scaled = pd.DataFrame(X_scaled, columns=features_to_scale)

# Gabungkan dengan kolom lainnya dari df_encoded
df_final = pd.concat([df_encoded.drop(columns=features_to_scale).reset_index(drop=True), df_scaled.reset_index(drop=True)], axis=1)

# Tampilkan hasil akhir
print(df_final.info())

"""kolom seperti SeniorCitizen, tenure, MonthlyCharges, dan TotalCharges sekarang sudah bertipe float64. Ini berarti bahwa kolom-kolom tersebut sudah siap untuk digunakan dalam analisis atau pelatihan model.

Tampilkan deskripsi statistik untuk keempat kolom yang distandarisasi
"""

df_final[features_to_scale].describe().round(4)

"""
*   Mean: Rata-rata nilai untuk semua fitur distandarisasi mendekati 0, menunjukkan bahwa data telah terskalakan dengan baik.

*   Standar Deviasi (std): Semua fitur memiliki standar deviasi sekitar 1, yang merupakan karakteristik dari data yang distandarisasi.

*  Min dan Max: Rentang nilai untuk setiap fitur menunjukkan variabilitas data setelah distandarisasi. Nilai ekstrem (min dan max) terdistribusi dengan baik, dan tidak ada outlier signifikan."""

df_final

"""Mengonversi kolom Churn ke format numerik"""

df_final['Churn'] = df_final['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

# Cek hasil konversi
print(df_final['Churn'].value_counts())

"""Konversi kolom Churn ke format numerik dilakukan untuk mempermudah analisis dan pelatihan model. Dalam konteks ini, nilai "Yes" diubah menjadi 1 (mengindikasikan pelanggan yang churn) dan nilai "No" diubah menjadi 0 (mengindikasikan pelanggan yang tidak churn).

* 0 (Tidak Churn): 5174 entri
* 1 (Churn): 1869 entri

Ketidakseimbangan Kelas: Terdapat lebih banyak entri untuk kelas 0 dibandingkan kelas 1. Ini dapat menjadi masalah jika model yang digunakan sangat dipengaruhi oleh jumlah contoh dari masing-masing kelas. kita akan mengatasinya dengan menambah jumlah dataset yang churn dengan teknik SMOTE

Menerapkan Teknik SMOTE

Dengan menghasilkan sampel sintetik untuk kelas minoritas, SMOTE meningkatkan representasi data tersebut, sehingga model dapat belajar dengan lebih baik dari contoh yang beragam. Ini membantu meningkatkan akurasi model dalam memprediksi kelas minoritas, memperbaiki metrik evaluasi, dan mencegah overfitting pada kelas mayoritas, yang pada akhirnya meningkatkan generalisasi model pada data yang tidak terlihat.
"""

# Cek jenis data dari kolom Churn
print(df_final['Churn'].dtype)

# Pisahkan fitur dan target
X = df_final.drop(columns=['Churn'])  # Fitur
y = df_final['Churn']  # Target yang sudah dalam format 0 dan 1

# Pisahkan menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Terapkan SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Tampilkan distribusi kelas sebelum dan sesudah SMOTE
print("Distribusi kelas sebelum SMOTE:")
print(y_train.value_counts())
print("\nDistribusi kelas setelah SMOTE:")
print(y_resampled.value_counts())

"""Sebelum menerapkan SMOTE, distribusi kelas dalam dataset menunjukkan ketidakseimbangan yang signifikan, di mana kelas '0' (tidak churn) memiliki 4139 entri, sementara kelas '1' (churn) hanya memiliki 1495 entri. Setelah menerapkan SMOTE, jumlah entri untuk kedua kelas menjadi seimbang, masing-masing memiliki 4139 entri. Hal ini mengindikasikan bahwa SMOTE berhasil menghasilkan sampel sintetik untuk kelas minoritas, meningkatkan representasi kelas tersebut dan membantu dalam pelatihan model yang lebih efektif dan seimbang.

menggabungkan kembali data yang telah di-resample menggunakan SMOTE ke dalam dataset asli
"""

# Buat DataFrame dari hasil SMOTE
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['Churn'] = y_resampled

# Gabungkan kembali dengan data pengujian
df_final_resampled = pd.concat([df_resampled, X_test.assign(Churn=y_test)], axis=0).reset_index(drop=True)

# Tampilkan informasi tentang DataFrame yang sudah disesuaikan
print("\nInformasi DataFrame yang sudah disesuaikan:")
print(df_final_resampled.info())

# Tampilkan distribusi kelas di DataFrame yang sudah disesuaikan
print("\nDistribusi kelas di DataFrame yang sudah disesuaikan:")
print(df_final_resampled['Churn'].value_counts())

"""Train-Test-Split"""

# Memisahkan fitur dan target dari df_final_resampled
X = df_final_resampled.drop(columns=['Churn'])  # Fitur
y = df_final_resampled['Churn']  # Target

# menggunakan 15% data untuk testing dan sisanya untuk training
test_size = 0.15
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)

# Cek jumlah sampel
print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""kita akan membuat model pakai XGBoost"""

# Inisialisasi model XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Melatih model
xgb_model.fit(X_train, y_train)

# Memprediksi data test
xgb_predictions = xgb_model.predict(X_test)

# Menghitung akurasi
xgb_accuracy = accuracy_score(y_test, xgb_predictions)
print(f'XGBoost Accuracy: {xgb_accuracy:.4f}')

"""Model XGBoost yang sudah dilatih berhasil mencapai akurasi sebesar 82.74% pada data pengujian. Ini menunjukkan bahwa model dapat memprediksi kelas dengan cukup baik,

Tuning

Dalam proses tuning model, digunakan Grid Search untuk meningkatkan akurasi. Grid Search adalah teknik yang memungkinkan pengembang untuk secara sistematis menguji berbagai kombinasi hyperparameter untuk model yang dipilih. Dengan mendefinisikan grid parameter, pengembang dapat mengevaluasi performa model pada setiap kombinasi tersebut menggunakan cross-validation. Pendekatan ini membantu dalam menemukan pengaturan parameter yang paling optimal, sehingga meningkatkan kemampuan model dalam memprediksi hasil di data yang tidak terlihat. Melalui Grid Search, diharapkan dapat mencapai akurasi yang lebih tinggi dan meningkatkan generalisasi model.
"""

# Mempersiapkan model XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Mempersiapkan parameter untuk grid search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 10, 15, 20],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'min_child_weight': [1, 2, 3]
}

# Melakukan hyperparameter tuning
grid_search = GridSearchCV(estimator=xgb_model,
                           param_grid=param_grid,
                           scoring='accuracy',
                           cv=5,
                           verbose=1,
                           n_jobs=-1)

# Melatih model dengan data pelatihan
grid_search.fit(X_train, y_train)

# Menampilkan parameter terbaik
print("Best parameters found: ", grid_search.best_params_)
print("Best accuracy found: ", grid_search.best_score_)

# Melatih model dengan parameter terbaik
best_xgb_model = grid_search.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Melakukan prediksi
y_pred_xgb = best_xgb_model.predict(X_test)

# Menampilkan akurasi model
xgb_accuracy = best_xgb_model.score(X_test, y_test)
print(f'Improved XGBoost Accuracy: {xgb_accuracy}')

"""Model yang dihasilkan dari parameter ini menunjukkan peningkatan akurasi menjadi 84.11% pada data pengujian. Peningkatan akurasi ini menunjukkan bahwa tuning parameter efektif dalam meningkatkan performa model.

Sekarang akan dilakukan confusion matrix untuk mengevaluasi performa model klasifikasi yang telah dilatih. Ini akan memberikan informasi detail tentang bagaimana model mengklasifikasikan data, termasuk seberapa baik model mendeteksi churn atau tidak churn dalam dataset pelanggan. Dengan ini, kita dapat melihat jumlah prediksi benar dan salah untuk setiap kelas (churn atau tidak churn), serta menghitung metrik seperti precision, recall, dan F1-score untuk analisis yang lebih mendalam.
"""

# Menghitung confusion matrix
cm = confusion_matrix(y_test, y_pred_xgb)

# Menampilkan confusion matrix
print("Confusion Matrix:")
print(cm)

# Menampilkan classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

"""Akurasi keseluruhan model adalah 0.84, menunjukkan bahwa model dapat memprediksi dengan benar 84% dari seluruh data uji. Hasil ini menunjukkan bahwa model XGBoost yang telah dituning memiliki performa yang baik dalam memprediksi churn pelanggan.







"""